# è¯ç‰©æ‰°åŠ¨é¢„æµ‹ä¼˜åŒ–è®¾è®¡æ–¹æ¡ˆ

## ç›®æ ‡ï¼šæå‡ pearson_delta æŒ‡æ ‡

---

## 1. é—®é¢˜åˆ†ææ€»ç»“

### 1.1 å½“å‰å®éªŒç»“æœ
- LoRA å¾®è°ƒå MSE/MAE æ˜¾è‘—ä¸‹é™ï¼ˆå¥½ï¼‰
- **pearson_delta ä¸‹é™äº† 13.47%**ï¼ˆä» 0.4428 é™è‡³ 0.3832ï¼‰- è¿™æ˜¯æ ¸å¿ƒé—®é¢˜

### 1.2 pearson_delta æŒ‡æ ‡ç†è§£

æ ¹æ® [`pearson_delta()`](state/.venv/lib/python3.11/site-packages/cell_eval/metrics/_anndata.py:24) çš„å®ç°ï¼š

```python
def pearson_delta(data: PerturbationAnndataPair, embed_key: str | None = None) -> dict[str, float]:
    """Compute Pearson correlation between mean differences from control."""
    return _generic_evaluation(data, pearsonr, use_delta=True, embed_key=embed_key)
```

å…³é”®ç‚¹ï¼š
- è®¡ç®—çš„æ˜¯**æ‰°åŠ¨æ•ˆåº”ï¼ˆdeltaï¼‰**çš„ Pearson ç›¸å…³æ€§
- `delta = perturbation_effect(which="pred/real")` = æ‰°åŠ¨åå‡å€¼ - æ§åˆ¶ç»„å‡å€¼
- **ä¸æ˜¯ç›´æ¥æ¯”è¾ƒé¢„æµ‹å€¼å’ŒçœŸå®å€¼ï¼Œè€Œæ˜¯æ¯”è¾ƒé¢„æµ‹çš„"å˜åŒ–é‡"å’ŒçœŸå®çš„"å˜åŒ–é‡"**

### 1.3 åŸºå› æ•²é™¤ vs è¯ç‰©æ‰°åŠ¨çš„å…³é”®å·®å¼‚

| ç‰¹æ€§     | åŸºå› æ•²é™¤       | è¯ç‰©æ‰°åŠ¨                           |
| -------- | -------------- | ---------------------------------- |
| æ‰°åŠ¨ç±»å‹ | ç¦»æ•£ã€äºŒå…ƒ     | è¿ç»­ï¼ˆä¸åŒæµ“åº¦ï¼‰                   |
| ä½œç”¨æœºåˆ¶ | å•åŸºå› é¶ç‚¹æ˜ç¡® | å¤šé€šè·¯ã€å¤šé¶ç‚¹                     |
| æ•ˆåº”å¼ºåº¦ | ç›¸å¯¹å›ºå®š       | å‰‚é‡ä¾èµ–                           |
| æ•°æ®åˆ†å¸ƒ | æ–¹å·®è¾ƒå°       | æ–¹å·®å¤§ï¼ŒåŒä¸€è¯ç‰©ä¸åŒæµ“åº¦æ•ˆåº”å·®å¼‚å¤§ |

### 1.4 ç°æœ‰ä»£ç çš„æ ¸å¿ƒé—®é¢˜

#### é—®é¢˜ 1ï¼šæ•°æ®é…å¯¹ç­–ç•¥ä¸é€‚åˆè¯ç‰©æ‰°åŠ¨

[`AnnDataPerturbationDataset.__getitem__()`](state/for_state/scripts/finetune.py:190):
```python
# éšæœºé€‰æ‹©æ§åˆ¶ç»„ç»†èƒ - é—®é¢˜æ‰€åœ¨ï¼
ctrl_idx = np.random.choice(self.ctrl_indices)
```

**é—®é¢˜**ï¼šéšæœºé…å¯¹å¿½ç•¥äº†æ‰¹æ¬¡æ•ˆåº”ã€ç»†èƒç±»å‹ç­‰æ··æ‚å› ç´ ã€‚

#### é—®é¢˜ 2ï¼šæŸå¤±å‡½æ•°æœªç›´æ¥ä¼˜åŒ– delta

[`StateTransitionPerturbationModel.training_step()`](state/src/state/tx/models/state_transition.py:500):
```python
# ç›´æ¥æ¯”è¾ƒ pred å’Œ targetï¼Œè€Œä¸æ˜¯ delta
main_loss = self.loss_fn(pred, target).nanmean()
```

**é—®é¢˜**ï¼šæŸå¤±å‡½æ•°ä¼˜åŒ–çš„æ˜¯ç»å¯¹è¡¨è¾¾å€¼ï¼Œè€Œä¸æ˜¯æ‰°åŠ¨æ•ˆåº”ã€‚

#### é—®é¢˜ 3ï¼šæ¨æ–­æ—¶ä½¿ç”¨æ‰°åŠ¨åç»†èƒè‡ªèº«çš„è¡¨è¾¾ä½œä¸ºè¾“å…¥

[`infer_lora.py`](state/for_state/scripts/infer_lora.py:299):
```python
# é—®é¢˜ï¼šä½¿ç”¨æ‰°åŠ¨åç»†èƒçš„è¡¨è¾¾ä½œä¸ºè¾“å…¥
X_batch = torch.tensor(X[start_idx:end_idx], dtype=torch.float32).to(device)
```

**é—®é¢˜**ï¼šè¯„ä¼°æ—¶åº”è¯¥ä½¿ç”¨æ§åˆ¶ç»„è¡¨è¾¾ä½œä¸ºè¾“å…¥ï¼Œæ‰èƒ½æ­£ç¡®è¯„ä¼°æ¨¡å‹é¢„æµ‹æ‰°åŠ¨æ•ˆåº”çš„èƒ½åŠ›ã€‚

---

## 2. ä¼˜åŒ–æ–¹æ¡ˆè®¾è®¡

### 2.1 æ•°æ®é…å¯¹ç­–ç•¥ä¼˜åŒ–

#### æ–¹æ¡ˆ Aï¼šåŸºäºæ‰¹æ¬¡çš„é…å¯¹ï¼ˆæ¨èï¼‰

```python
class AnnDataPerturbationDatasetV2(Dataset):
    """æ”¹è¿›çš„æ•°æ®é›†ç±»ï¼Œæ”¯æŒåŸºäºæ‰¹æ¬¡çš„æ™ºèƒ½é…å¯¹"""
    
    def __init__(self, adata, pert_onehot_map: Dict[str, torch.Tensor], 
                 pert_col: str = "drugname_drugconc",
                 control_label: str = "[('DMSO_TF', 0.0, 'uM')]",
                 batch_col: str = None,           # æ–°å¢ï¼šæ‰¹æ¬¡åˆ—
                 celltype_col: str = None,        # æ–°å¢ï¼šç»†èƒç±»å‹åˆ—
                 pairing_strategy: str = "batch"  # æ–°å¢ï¼šé…å¯¹ç­–ç•¥
                ):
        """
        Args:
            pairing_strategy: 
                - "random": éšæœºé…å¯¹ï¼ˆåŸå§‹è¡Œä¸ºï¼‰
                - "batch": åŒæ‰¹æ¬¡é…å¯¹
                - "celltype": åŒç»†èƒç±»å‹é…å¯¹
                - "batch_celltype": åŒæ‰¹æ¬¡+åŒç»†èƒç±»å‹é…å¯¹
        """
        self.adata = adata
        self.pert_col = pert_col
        self.control_label = control_label
        self.batch_col = batch_col
        self.celltype_col = celltype_col
        self.pairing_strategy = pairing_strategy
        
        # æ„å»ºé…å¯¹ç´¢å¼•æ˜ å°„
        self._build_pairing_index()
    
    def _build_pairing_index(self):
        """æ„å»ºæ§åˆ¶ç»„ç»†èƒçš„åˆ†ç»„ç´¢å¼•ï¼Œç”¨äºæ™ºèƒ½é…å¯¹"""
        pert_names = self.adata.obs[self.pert_col].values
        
        # è¯†åˆ«æ§åˆ¶ç»„å’Œæ‰°åŠ¨ç»„
        self.ctrl_mask = pert_names == self.control_label
        self.pert_mask = pert_names != self.control_label
        
        self.ctrl_indices = np.where(self.ctrl_mask)[0]
        self.pert_indices = np.where(self.pert_mask)[0]
        
        if self.pairing_strategy == "random":
            self.ctrl_groups = None
            return
        
        # æ„å»ºåˆ†ç»„ç´¢å¼•
        self.ctrl_groups = {}
        
        for idx in self.ctrl_indices:
            group_key = self._get_group_key(idx)
            if group_key not in self.ctrl_groups:
                self.ctrl_groups[group_key] = []
            self.ctrl_groups[group_key].append(idx)
        
        # è½¬æ¢ä¸º numpy æ•°ç»„ä»¥åŠ é€Ÿé‡‡æ ·
        for key in self.ctrl_groups:
            self.ctrl_groups[key] = np.array(self.ctrl_groups[key])
    
    def _get_group_key(self, idx):
        """è·å–ç»†èƒçš„åˆ†ç»„é”®"""
        keys = []
        if self.pairing_strategy in ["batch", "batch_celltype"]:
            if self.batch_col and self.batch_col in self.adata.obs:
                keys.append(str(self.adata.obs[self.batch_col].iloc[idx]))
        if self.pairing_strategy in ["celltype", "batch_celltype"]:
            if self.celltype_col and self.celltype_col in self.adata.obs:
                keys.append(str(self.adata.obs[self.celltype_col].iloc[idx]))
        return tuple(keys) if keys else ("default",)
    
    def _get_paired_ctrl_idx(self, pert_idx):
        """ä¸ºæ‰°åŠ¨ç»„ç»†èƒè·å–é…å¯¹çš„æ§åˆ¶ç»„ç»†èƒç´¢å¼•"""
        if self.pairing_strategy == "random" or self.ctrl_groups is None:
            return np.random.choice(self.ctrl_indices)
        
        group_key = self._get_group_key(pert_idx)
        
        if group_key in self.ctrl_groups and len(self.ctrl_groups[group_key]) > 0:
            return np.random.choice(self.ctrl_groups[group_key])
        else:
            # å›é€€åˆ°éšæœºé‡‡æ ·
            return np.random.choice(self.ctrl_indices)
    
    def __getitem__(self, idx):
        pert_idx = self.pert_indices[idx]
        ctrl_idx = self._get_paired_ctrl_idx(pert_idx)
        
        # ... å…¶ä½™é€»è¾‘ä¿æŒä¸å˜
```

#### æ–¹æ¡ˆ Bï¼šæ§åˆ¶ç»„å‡å€¼é…å¯¹

```python
class AnnDataPerturbationDatasetMean(Dataset):
    """ä½¿ç”¨æ§åˆ¶ç»„å‡å€¼ä½œä¸ºè¾“å…¥çš„æ•°æ®é›†"""
    
    def __init__(self, adata, pert_onehot_map, pert_col, control_label,
                 use_ctrl_mean: bool = True,      # æ˜¯å¦ä½¿ç”¨æ§åˆ¶ç»„å‡å€¼
                 batch_col: str = None):
        # ...
        
        if use_ctrl_mean:
            self._compute_ctrl_means()
    
    def _compute_ctrl_means(self):
        """é¢„è®¡ç®—åˆ†ç»„æ§åˆ¶ç»„å‡å€¼"""
        ctrl_data = self.adata[self.ctrl_mask]
        
        if self.batch_col:
            # æŒ‰æ‰¹æ¬¡è®¡ç®—æ§åˆ¶ç»„å‡å€¼
            self.ctrl_means = {}
            for batch in ctrl_data.obs[self.batch_col].unique():
                batch_mask = ctrl_data.obs[self.batch_col] == batch
                self.ctrl_means[batch] = np.mean(ctrl_data[batch_mask].X, axis=0)
        else:
            # å…¨å±€æ§åˆ¶ç»„å‡å€¼
            self.ctrl_mean = np.mean(ctrl_data.X, axis=0)
    
    def __getitem__(self, idx):
        pert_idx = self.pert_indices[idx]
        
        # ä½¿ç”¨å¯¹åº”æ‰¹æ¬¡çš„æ§åˆ¶ç»„å‡å€¼
        if self.batch_col and hasattr(self, 'ctrl_means'):
            batch = self.adata.obs[self.batch_col].iloc[pert_idx]
            ctrl_emb = torch.tensor(self.ctrl_means[batch], dtype=torch.float32)
        else:
            ctrl_emb = torch.tensor(self.ctrl_mean, dtype=torch.float32)
        
        # ...
```

---

### 2.2 Delta-Aware æŸå¤±å‡½æ•°è®¾è®¡

#### æ–¹æ¡ˆ Aï¼šç›´æ¥ä¼˜åŒ– Delta çš„ Pearson ç›¸å…³æ€§ï¼ˆæ¨èï¼‰

```python
class DeltaAwareLoss(nn.Module):
    """
    ç›´æ¥ä¼˜åŒ–æ‰°åŠ¨æ•ˆåº”(delta)çš„æŸå¤±å‡½æ•°
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    - delta_pred = pred - ctrl
    - delta_true = target - ctrl  
    - ä¼˜åŒ– delta_pred å’Œ delta_true ä¹‹é—´çš„ç›¸å…³æ€§
    """
    
    def __init__(self, 
                 base_loss_weight: float = 1.0,    # åŸå§‹æŸå¤±æƒé‡
                 delta_mse_weight: float = 1.0,    # Delta MSEæƒé‡
                 delta_pearson_weight: float = 0.5, # Delta Pearsonç›¸å…³æ€§æƒé‡
                 eps: float = 1e-8):
        super().__init__()
        self.base_loss_weight = base_loss_weight
        self.delta_mse_weight = delta_mse_weight
        self.delta_pearson_weight = delta_pearson_weight
        self.eps = eps
        self.mse_loss = nn.MSELoss()
    
    def pearson_correlation_loss(self, pred, target):
        """
        è®¡ç®— Pearson ç›¸å…³æ€§æŸå¤±
        è¿”å› 1 - correlationï¼Œä½¿å¾—æœ€å°åŒ–æŸå¤±ç­‰ä»·äºæœ€å¤§åŒ–ç›¸å…³æ€§
        """
        # å±•å¹³ä¸º [batch, features]
        pred_flat = pred.reshape(pred.size(0), -1)
        target_flat = target.reshape(target.size(0), -1)
        
        # ä¸­å¿ƒåŒ–
        pred_centered = pred_flat - pred_flat.mean(dim=1, keepdim=True)
        target_centered = target_flat - target_flat.mean(dim=1, keepdim=True)
        
        # è®¡ç®—ç›¸å…³æ€§
        numerator = (pred_centered * target_centered).sum(dim=1)
        pred_std = torch.sqrt((pred_centered ** 2).sum(dim=1) + self.eps)
        target_std = torch.sqrt((target_centered ** 2).sum(dim=1) + self.eps)
        
        correlation = numerator / (pred_std * target_std + self.eps)
        
        # è¿”å› 1 - mean(correlation) ä½œä¸ºæŸå¤±
        return 1 - correlation.mean()
    
    def forward(self, pred, target, ctrl, base_loss=None):
        """
        Args:
            pred: é¢„æµ‹çš„æ‰°åŠ¨åè¡¨è¾¾ [B, S, D]
            target: çœŸå®çš„æ‰°åŠ¨åè¡¨è¾¾ [B, S, D]
            ctrl: æ§åˆ¶ç»„è¡¨è¾¾ [B, S, D]
            base_loss: åŸå§‹æŸå¤±ï¼ˆå¦‚ OT lossï¼‰
        """
        # è®¡ç®— delta
        delta_pred = pred - ctrl
        delta_true = target - ctrl
        
        total_loss = 0.0
        
        # 1. åŸå§‹æŸå¤±ï¼ˆå¦‚ OT lossï¼‰
        if base_loss is not None and self.base_loss_weight > 0:
            total_loss = total_loss + self.base_loss_weight * base_loss
        
        # 2. Delta MSE æŸå¤±
        if self.delta_mse_weight > 0:
            delta_mse = self.mse_loss(delta_pred, delta_true)
            total_loss = total_loss + self.delta_mse_weight * delta_mse
        
        # 3. Delta Pearson ç›¸å…³æ€§æŸå¤±
        if self.delta_pearson_weight > 0:
            delta_pearson_loss = self.pearson_correlation_loss(delta_pred, delta_true)
            total_loss = total_loss + self.delta_pearson_weight * delta_pearson_loss
        
        return total_loss
```

#### æ–¹æ¡ˆ Bï¼šé›†æˆåˆ° `training_step` ä¸­

```python
def training_step_with_delta_loss(self, batch: Dict[str, torch.Tensor], 
                                   batch_idx: int, padded=True) -> torch.Tensor:
    """æ”¹è¿›çš„è®­ç»ƒæ­¥éª¤ï¼ŒåŠ å…¥ delta-aware æŸå¤±"""
    
    # è·å–é¢„æµ‹
    pred = self.forward(batch, padded=padded)
    target = batch["pert_cell_emb"]
    ctrl = batch["ctrl_cell_emb"]
    
    if padded:
        pred = pred.reshape(-1, self.cell_sentence_len, self.output_dim)
        target = target.reshape(-1, self.cell_sentence_len, self.output_dim)
        ctrl = ctrl.reshape(-1, self.cell_sentence_len, self.output_dim)
    else:
        pred = pred.reshape(1, -1, self.output_dim)
        target = target.reshape(1, -1, self.output_dim)
        ctrl = ctrl.reshape(1, -1, self.output_dim)
    
    # 1. åŸå§‹ OT æŸå¤±
    main_loss = self.loss_fn(pred, target).nanmean()
    
    # 2. Delta MSE æŸå¤±ï¼ˆæ–°å¢ï¼‰
    delta_pred = pred - ctrl
    delta_true = target - ctrl
    delta_mse_loss = F.mse_loss(delta_pred, delta_true)
    
    # 3. Delta Pearson æŸå¤±ï¼ˆæ–°å¢ï¼‰
    delta_pearson_loss = self._compute_pearson_loss(delta_pred, delta_true)
    
    # åŠ æƒç»„åˆ
    total_loss = (
        self.hparams.get("base_loss_weight", 1.0) * main_loss +
        self.hparams.get("delta_mse_weight", 0.5) * delta_mse_loss +
        self.hparams.get("delta_pearson_weight", 0.3) * delta_pearson_loss
    )
    
    # æ—¥å¿—è®°å½•
    self.log("train/main_loss", main_loss)
    self.log("train/delta_mse_loss", delta_mse_loss)
    self.log("train/delta_pearson_loss", delta_pearson_loss)
    self.log("train_loss", total_loss)
    
    return total_loss

def _compute_pearson_loss(self, pred, target, eps=1e-8):
    """è®¡ç®— Pearson ç›¸å…³æ€§æŸå¤±"""
    # æŒ‰æ‰¹æ¬¡è®¡ç®—
    B, S, D = pred.shape
    pred_flat = pred.reshape(B, -1)  # [B, S*D]
    target_flat = target.reshape(B, -1)
    
    # ä¸­å¿ƒåŒ–
    pred_centered = pred_flat - pred_flat.mean(dim=1, keepdim=True)
    target_centered = target_flat - target_flat.mean(dim=1, keepdim=True)
    
    # Pearson ç›¸å…³ç³»æ•°
    numerator = (pred_centered * target_centered).sum(dim=1)
    pred_var = (pred_centered ** 2).sum(dim=1)
    target_var = (target_centered ** 2).sum(dim=1)
    
    correlation = numerator / (torch.sqrt(pred_var * target_var) + eps)
    
    # è¿”å› 1 - mean_correlation ä½œä¸ºæŸå¤±
    return 1 - correlation.mean()
```

---

### 2.3 LoRA åº”ç”¨èŒƒå›´æ‰©å±•

#### å½“å‰çŠ¶æ€

[`finetune.py`](state/for_state/scripts/finetune.py:469) ä¸­ä»…å¯¹ `transformer_backbone` åº”ç”¨ LoRAï¼š

```python
model, lora_modules = replace_linear_with_lora(model, r=args.lora_rank, alpha=args.lora_alpha,
                                               target_module_keywords=target_keywords)
```

#### æ–¹æ¡ˆï¼šæ‰©å±•åˆ° pert_encoder å’Œ basal_encoder

```python
def apply_lora_to_encoders(model, lora_rank, lora_alpha):
    """
    å¯¹ pert_encoder å’Œ basal_encoder åº”ç”¨ LoRA
    
    èƒŒæ™¯ï¼šå¯¹äºè¯ç‰©æ‰°åŠ¨ï¼Œè¯ç‰©ç¼–ç å¯èƒ½éœ€è¦é€‚é…æ–°çš„è¯ç‰©-ç»†èƒç›¸äº’ä½œç”¨æ¨¡å¼
    """
    lora_modules = {}
    
    # 1. å¯¹ transformer_backbone åº”ç”¨ LoRAï¼ˆåŸæœ‰é€»è¾‘ï¼‰
    model, backbone_lora = replace_linear_with_lora(
        model, r=lora_rank, alpha=lora_alpha,
        target_module_keywords=["transformer_backbone", "attn", "mlp"]
    )
    lora_modules.update(backbone_lora)
    
    # 2. å¯¹ pert_encoder åº”ç”¨ LoRAï¼ˆæ–°å¢ - é‡è¦ï¼ï¼‰
    # è¯ç‰©æ‰°åŠ¨å¯èƒ½éœ€è¦ä¸åŒçš„ç¼–ç æ–¹å¼
    model, pert_lora = replace_linear_with_lora(
        model.pert_encoder, r=lora_rank, alpha=lora_alpha
    )
    for name, module in pert_lora.items():
        lora_modules[f"pert_encoder.{name}"] = module
    
    # 3. å¯é€‰ï¼šå¯¹ basal_encoder åº”ç”¨è¾ƒå° rank çš„ LoRA
    # åŸºç¡€è¡¨è¾¾ç¼–ç é€šå¸¸å˜åŒ–ä¸å¤§
    model, basal_lora = replace_linear_with_lora(
        model.basal_encoder, r=lora_rank // 2, alpha=lora_alpha / 2
    )
    for name, module in basal_lora.items():
        lora_modules[f"basal_encoder.{name}"] = module
    
    return model, lora_modules
```

#### æ¨èçš„ LoRA åº”ç”¨ç­–ç•¥

| æ¨¡å—                 | æ˜¯å¦åº”ç”¨ LoRA | Rank | Alpha | åŸå›                    |
| -------------------- | ------------- | ---- | ----- | ---------------------- |
| transformer_backbone | âœ… æ˜¯          | 8    | 1.0   | æ ¸å¿ƒé¢„æµ‹æ¨¡å—           |
| pert_encoder         | âœ… æ˜¯ï¼ˆæ¨èï¼‰  | 8    | 1.0   | è¯ç‰©ç¼–ç éœ€è¦é€‚é…æ–°æ•°æ® |
| basal_encoder        | âš ï¸ å¯é€‰        | 4    | 0.5   | åŸºç¡€è¡¨è¾¾ç¼–ç å˜åŒ–è¾ƒå°   |
| project_out          | âŒ å¦          | -    | -     | è¾“å‡ºæŠ•å½±å±‚ä¿æŒä¸å˜     |

---

### 2.4 æ¨æ–­æµç¨‹æ”¹è¿›

#### æ ¸å¿ƒé—®é¢˜

å½“å‰æ¨æ–­ä½¿ç”¨æ‰°åŠ¨åç»†èƒçš„è¡¨è¾¾ä½œä¸ºè¾“å…¥ï¼š
```python
X_batch = torch.tensor(X[start_idx:end_idx], dtype=torch.float32)
```

è¿™æ˜¯**é”™è¯¯çš„**ï¼è¯„ä¼° pearson_delta æ—¶ï¼Œåº”è¯¥ï¼š
1. ä½¿ç”¨æ§åˆ¶ç»„è¡¨è¾¾ä½œä¸ºè¾“å…¥
2. è®©æ¨¡å‹é¢„æµ‹æ‰°åŠ¨åçš„è¡¨è¾¾
3. è®¡ç®—é¢„æµ‹çš„ delta å’ŒçœŸå®çš„ delta çš„ç›¸å…³æ€§

#### æ”¹è¿›æ–¹æ¡ˆ

```python
def run_tx_infer_v2(args):
    """æ”¹è¿›çš„æ¨æ–­æµç¨‹ï¼Œæ­£ç¡®å¤„ç†è¯ç‰©æ‰°åŠ¨è¯„ä¼°"""
    
    # ... æ¨¡å‹åŠ è½½ä»£ç ä¿æŒä¸å˜ ...
    
    # å…³é”®æ”¹è¿›ï¼šä½¿ç”¨æ§åˆ¶ç»„å‡å€¼ä½œä¸ºè¾“å…¥
    logger.info("Computing control group mean expression...")
    ctrl_mask = adata.obs[args.pert_col] == control_pert
    ctrl_data = adata[ctrl_mask]
    
    if args.embed_key in ctrl_data.obsm:
        ctrl_X = ctrl_data.obsm[args.embed_key]
    else:
        ctrl_X = ctrl_data.X.toarray() if hasattr(ctrl_data.X, 'toarray') else ctrl_data.X
    
    # è®¡ç®—å…¨å±€æ§åˆ¶ç»„å‡å€¼ï¼Œæˆ–æŒ‰æ‰¹æ¬¡è®¡ç®—
    if args.batch_col and args.batch_col in adata.obs:
        ctrl_means = {}
        for batch in ctrl_data.obs[args.batch_col].unique():
            batch_mask = ctrl_data.obs[args.batch_col] == batch
            ctrl_means[batch] = np.mean(ctrl_X[batch_mask], axis=0)
        logger.info(f"Computed control means for {len(ctrl_means)} batches")
    else:
        ctrl_mean = np.mean(ctrl_X, axis=0)
        ctrl_means = None
        logger.info("Computed global control mean")
    
    # åªå¯¹æ‰°åŠ¨ç»„ç»†èƒè¿›è¡Œæ¨æ–­
    pert_mask = adata.obs[args.pert_col] != control_pert
    adata_pert = adata[pert_mask].copy()
    
    all_preds = []
    n_samples = adata_pert.n_obs
    
    with torch.no_grad():
        for batch_idx in range(0, n_samples, batch_size):
            start_idx = batch_idx
            end_idx = min(start_idx + batch_size, n_samples)
            current_batch_size = end_idx - start_idx
            
            # è·å–è¿™æ‰¹ç»†èƒçš„æ§åˆ¶ç»„è¾“å…¥
            if ctrl_means is not None:
                # æŒ‰æ‰¹æ¬¡è·å–å¯¹åº”çš„æ§åˆ¶ç»„å‡å€¼
                batch_labels = adata_pert.obs[args.batch_col].iloc[start_idx:end_idx].values
                ctrl_batch = np.stack([ctrl_means.get(b, ctrl_mean) for b in batch_labels])
            else:
                # ä½¿ç”¨å…¨å±€æ§åˆ¶ç»„å‡å€¼
                ctrl_batch = np.tile(ctrl_mean, (current_batch_size, 1))
            
            X_batch = torch.tensor(ctrl_batch, dtype=torch.float32).to(device)
            
            # è·å–æ‰°åŠ¨ç¼–ç 
            pert_batch = pert_tensor[start_idx:end_idx].to(device)
            
            # ... å¡«å……å’Œæ¨æ–­é€»è¾‘ ...
            
            batch = {
                "ctrl_cell_emb": X_batch,  # ä½¿ç”¨æ§åˆ¶ç»„è¡¨è¾¾ï¼
                "pert_emb": pert_batch,
                "pert_name": pert_names_batch,
                "batch": torch.zeros((1, cell_sentence_len), device=device),
            }
            
            batch_preds = model.predict_step(batch, batch_idx=batch_idx, padded=False)
            # ...
    
    # è¾“å‡ºåŒ…å«é¢„æµ‹çš„æ‰°åŠ¨åè¡¨è¾¾
    adata_pert.layers["predicted"] = np.concatenate(all_preds, axis=0)
    adata_pert.write_h5ad(output_path)
```

---

### 2.5 æ•°æ®å¢å¼ºä¸æ­£åˆ™åŒ–

#### æ–¹æ¡ˆ Aï¼šè¯ç‰©æµ“åº¦æ„ŸçŸ¥çš„å¢å¼º

```python
class DrugConcentrationAugmentation:
    """
    è¯ç‰©æµ“åº¦æ„ŸçŸ¥çš„æ•°æ®å¢å¼º
    
    æ€è·¯ï¼šåŒä¸€è¯ç‰©ä¸åŒæµ“åº¦äº§ç”Ÿçš„æ•ˆåº”æ˜¯è¿ç»­çš„ï¼Œå¯ä»¥è¿›è¡Œæ’å€¼å¢å¼º
    """
    
    def __init__(self, adata, pert_col, drug_col="drug_name", conc_col="concentration"):
        self.adata = adata
        self.pert_col = pert_col
        self.drug_col = drug_col
        self.conc_col = conc_col
        self._build_drug_concentration_map()
    
    def _build_drug_concentration_map(self):
        """æ„å»ºè¯ç‰©-æµ“åº¦-ç»†èƒæ˜ å°„"""
        self.drug_conc_cells = {}
        for drug in self.adata.obs[self.drug_col].unique():
            drug_data = self.adata[self.adata.obs[self.drug_col] == drug]
            concentrations = sorted(drug_data.obs[self.conc_col].unique())
            self.drug_conc_cells[drug] = {
                conc: np.where(
                    (self.adata.obs[self.drug_col] == drug) & 
                    (self.adata.obs[self.conc_col] == conc)
                )[0]
                for conc in concentrations
            }
    
    def interpolate_concentration(self, drug, target_conc):
        """
        ä¸ºç›®æ ‡æµ“åº¦ç”Ÿæˆæ’å€¼çš„ç»†èƒè¡¨è¾¾
        """
        if drug not in self.drug_conc_cells:
            return None
        
        concs = sorted(self.drug_conc_cells[drug].keys())
        
        # æ‰¾åˆ°ç›¸é‚»çš„æµ“åº¦
        lower_conc = max([c for c in concs if c <= target_conc], default=concs[0])
        upper_conc = min([c for c in concs if c >= target_conc], default=concs[-1])
        
        if lower_conc == upper_conc:
            # ç›´æ¥è¿”å›è¯¥æµ“åº¦çš„æ•°æ®
            return self.adata.X[self.drug_conc_cells[drug][lower_conc]]
        
        # çº¿æ€§æ’å€¼
        alpha = (target_conc - lower_conc) / (upper_conc - lower_conc)
        lower_expr = self.adata.X[self.drug_conc_cells[drug][lower_conc]].mean(axis=0)
        upper_expr = self.adata.X[self.drug_conc_cells[drug][upper_conc]].mean(axis=0)
        
        return (1 - alpha) * lower_expr + alpha * upper_expr
```

#### æ–¹æ¡ˆ Bï¼šè¯ç‰©è¿‡æ‹Ÿåˆæ­£åˆ™åŒ–

```python
def add_drug_diversity_regularization(self, pred, target, pert_names, ctrl):
    """
    é˜²æ­¢æ¨¡å‹å¯¹ç‰¹å®šè¯ç‰©è¿‡æ‹Ÿåˆçš„æ­£åˆ™åŒ–
    
    æ€è·¯ï¼šé¼“åŠ±æ¨¡å‹é¢„æµ‹æ›´å¤šæ ·åŒ–çš„è¯ç‰©æ•ˆåº”ï¼Œè€Œä¸æ˜¯è¶‹å‘äºæŸç§"å¹³å‡æ•ˆåº”"
    """
    # æŒ‰è¯ç‰©åˆ†ç»„è®¡ç®—é¢„æµ‹çš„ delta
    unique_drugs = set(pert_names)
    drug_deltas = {}
    
    for drug in unique_drugs:
        drug_mask = [name == drug for name in pert_names]
        if sum(drug_mask) > 0:
            drug_pred = pred[drug_mask]
            drug_ctrl = ctrl[drug_mask]
            drug_deltas[drug] = (drug_pred - drug_ctrl).mean(dim=0)
    
    if len(drug_deltas) < 2:
        return 0.0
    
    # è®¡ç®—è¯ç‰©é—´ delta çš„æ–¹å·® - é¼“åŠ±å¤šæ ·æ€§
    drug_delta_tensor = torch.stack(list(drug_deltas.values()))
    diversity = drug_delta_tensor.var(dim=0).mean()
    
    # è¿”å›è´Ÿçš„æ–¹å·®ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼ˆæœ€å¤§åŒ–æ–¹å·® = æœ€å°åŒ–è´Ÿæ–¹å·®ï¼‰
    return -0.1 * diversity
```

---

## 3. æ¨èçš„å®éªŒé…ç½®

### 3.1 ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é…å¯¹ä¼˜åŒ–

```bash
python finetune.py \
    --model_dir /path/to/model \
    --adata /path/to/c37.h5ad \
    --pert_col drugname_drugconc \
    --pairing_strategy batch \       # æ–°å¢ï¼šä½¿ç”¨æ‰¹æ¬¡é…å¯¹
    --batch_col batch \              # æ–°å¢ï¼šæŒ‡å®šæ‰¹æ¬¡åˆ—
    --epochs 5 \
    --batch_size 128 \
    --lr 5e-4 \
    --lora_rank 8
```

### 3.2 ç¬¬äºŒé˜¶æ®µï¼šDelta-Aware æŸå¤±

```bash
python finetune.py \
    --model_dir /path/to/model \
    --adata /path/to/c37.h5ad \
    --pert_col drugname_drugconc \
    --pairing_strategy batch \
    --use_delta_loss \               # æ–°å¢ï¼šå¯ç”¨ delta æŸå¤±
    --delta_mse_weight 0.5 \         # æ–°å¢ï¼šdelta MSE æƒé‡
    --delta_pearson_weight 0.3 \     # æ–°å¢ï¼šdelta Pearson æƒé‡
    --base_loss_weight 1.0 \
    --epochs 10 \
    --lr 3e-4
```

### 3.3 ç¬¬ä¸‰é˜¶æ®µï¼šæ‰©å±• LoRA

```bash
python finetune.py \
    --model_dir /path/to/model \
    --adata /path/to/c37.h5ad \
    --pert_col drugname_drugconc \
    --pairing_strategy batch \
    --use_delta_loss \
    --lora_target_modules transformer_backbone,pert_encoder \  # æ–°å¢ï¼šæ‰©å±• LoRA
    --lora_rank 8 \
    --epochs 10 \
    --lr 3e-4
```

### 3.4 æ¨èçš„è¶…å‚æ•°æœç´¢èŒƒå›´

| å‚æ•°                   | æœç´¢èŒƒå›´    | è¯´æ˜               |
| ---------------------- | ----------- | ------------------ |
| `lr`                   | 1e-4 ~ 1e-3 | å­¦ä¹ ç‡             |
| `lora_rank`            | 4, 8, 16    | LoRA ç§©            |
| `delta_mse_weight`     | 0.1 ~ 1.0   | Delta MSE æƒé‡     |
| `delta_pearson_weight` | 0.1 ~ 0.5   | Delta Pearson æƒé‡ |
| `base_loss_weight`     | 0.5 ~ 1.0   | åŸå§‹æŸå¤±æƒé‡       |

---

## 4. å®ç°ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | æ”¹è¿›é¡¹                         | é¢„æœŸæ”¶ç›Š         | å®ç°éš¾åº¦ |
| ------ | ------------------------------ | ---------------- | -------- |
| ğŸ”´ é«˜   | æ¨æ–­æµç¨‹ä¿®æ­£ï¼ˆä½¿ç”¨æ§åˆ¶ç»„å‡å€¼ï¼‰ | ç›´æ¥ä¿®å¤è¯„ä¼°é€»è¾‘ | ä½       |
| ğŸ”´ é«˜   | æ•°æ®é…å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆæ‰¹æ¬¡é…å¯¹ï¼‰   | å‡å°‘æ··æ‚å› ç´      | ä¸­       |
| ğŸŸ¡ ä¸­   | Delta-Aware æŸå¤±å‡½æ•°           | ç›´æ¥ä¼˜åŒ–ç›®æ ‡æŒ‡æ ‡ | ä¸­       |
| ğŸŸ¡ ä¸­   | LoRA æ‰©å±•åˆ° pert_encoder       | æå‡è¯ç‰©é€‚é…èƒ½åŠ› | ä½       |
| ğŸŸ¢ ä½   | è¯ç‰©æµ“åº¦å¢å¼º                   | æ•°æ®å¢å¼º         | é«˜       |

---

## 5. å·¥ä½œæµç¨‹å›¾

```mermaid
graph TD
    A[åŸå§‹æ•°æ® c37.h5ad] --> B[æ•°æ®é¢„å¤„ç†]
    B --> C{é…å¯¹ç­–ç•¥}
    C -->|æ‰¹æ¬¡é…å¯¹| D[æ‰¹æ¬¡åŒ¹é…çš„æ§åˆ¶ç»„-æ‰°åŠ¨ç»„å¯¹]
    C -->|éšæœºé…å¯¹| E[éšæœºæ§åˆ¶ç»„-æ‰°åŠ¨ç»„å¯¹]
    
    D --> F[è®­ç»ƒæ•°æ®]
    E --> F
    
    F --> G[LoRA å¾®è°ƒ]
    
    G --> H{æŸå¤±å‡½æ•°}
    H -->|åŸå§‹ OT Loss| I[main_loss]
    H -->|Delta MSE| J[delta_mse_loss]
    H -->|Delta Pearson| K[delta_pearson_loss]
    
    I --> L[åŠ æƒæ€»æŸå¤±]
    J --> L
    K --> L
    
    L --> M[åå‘ä¼ æ’­]
    M --> N[æ›´æ–° LoRA å‚æ•°]
    
    N --> O[æ¨æ–­]
    O -->|æ§åˆ¶ç»„å‡å€¼è¾“å…¥| P[é¢„æµ‹æ‰°åŠ¨åè¡¨è¾¾]
    P --> Q[è®¡ç®— pearson_delta]
```

---

## 6. æ–‡ä»¶ä¿®æ”¹æ¸…å•

### 6.1 [`finetune.py`](state/for_state/scripts/finetune.py) ä¿®æ”¹

1. **Line 132-186**: é‡å†™ `AnnDataPerturbationDataset` ç±»
   - æ·»åŠ  `pairing_strategy` å‚æ•°
   - å®ç° `_build_pairing_index()` æ–¹æ³•
   - å®ç° `_get_paired_ctrl_idx()` æ–¹æ³•

2. **Line 274-373**: ä¿®æ”¹ `train_lora()` å‡½æ•°
   - æ·»åŠ  `DeltaAwareLoss` æ”¯æŒ
   - æ·»åŠ  delta æŸå¤±çš„æ—¥å¿—è®°å½•

3. **Line 469-476**: æ‰©å±• LoRA åº”ç”¨èŒƒå›´
   - æ”¯æŒ `--lora_target_modules` å‚æ•°
   - é»˜è®¤åŒ…å« `pert_encoder`

### 6.2 [`infer_lora.py`](state/for_state/scripts/infer_lora.py) ä¿®æ”¹

1. **Line 219-250**: ä¿®æ”¹æ•°æ®åŠ è½½é€»è¾‘
   - åˆ†ç¦»æ§åˆ¶ç»„å’Œæ‰°åŠ¨ç»„
   - è®¡ç®—æ§åˆ¶ç»„å‡å€¼

2. **Line 293-340**: é‡å†™æ¨æ–­å¾ªç¯
   - ä½¿ç”¨æ§åˆ¶ç»„å‡å€¼ä½œä¸ºè¾“å…¥
   - æ­£ç¡®å¤„ç†æ‰¹æ¬¡ä¿¡æ¯

### 6.3 æ–°å¢æ–‡ä»¶

- `state/for_state/scripts/losses/delta_aware_loss.py`: Delta-Aware æŸå¤±å‡½æ•°å®ç°
- `state/for_state/scripts/datasets/drug_perturbation_dataset.py`: æ”¹è¿›çš„æ•°æ®é›†ç±»

---

## 7. æ€»ç»“

æœ¬è®¾è®¡æ–¹æ¡ˆé’ˆå¯¹è¯ç‰©æ‰°åŠ¨é¢„æµ‹ä»»åŠ¡ï¼Œä»ä»¥ä¸‹å››ä¸ªå…³é”®æ–¹é¢è¿›è¡Œä¼˜åŒ–ï¼š

1. **æ•°æ®é…å¯¹ç­–ç•¥**ï¼šä»éšæœºé…å¯¹æ”¹ä¸ºåŸºäºæ‰¹æ¬¡/ç»†èƒç±»å‹çš„æ™ºèƒ½é…å¯¹ï¼Œå‡å°‘æ··æ‚å› ç´ 
2. **æŸå¤±å‡½æ•°**ï¼šå¼•å…¥ Delta-Aware æŸå¤±ï¼Œç›´æ¥ä¼˜åŒ–æ‰°åŠ¨æ•ˆåº”çš„ç›¸å…³æ€§
3. **LoRA åº”ç”¨èŒƒå›´**ï¼šæ‰©å±•åˆ° `pert_encoder`ï¼Œæå‡è¯ç‰©ç¼–ç çš„é€‚é…èƒ½åŠ›
4. **æ¨æ–­æµç¨‹**ï¼šä¿®æ­£ä¸ºä½¿ç”¨æ§åˆ¶ç»„å‡å€¼ä½œä¸ºè¾“å…¥ï¼Œä¸è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—æ–¹å¼ä¿æŒä¸€è‡´

é¢„æœŸé€šè¿‡è¿™äº›æ”¹è¿›ï¼Œpearson_delta æŒ‡æ ‡èƒ½å¤Ÿæ¢å¤ç”šè‡³è¶…è¿‡åŸå§‹æ¨¡å‹çš„æ°´å¹³ï¼ˆ0.4428+ï¼‰ã€‚
