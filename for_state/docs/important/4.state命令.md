# state命令大全

## state tx 命令

1. state tx train

```bash
Hydra Configuration Help
==================================================
Configuration for method: tx

Full configuration structure:
data:
  name: PerturbationDataModule
  kwargs:
    toml_config_path: null
    embed_key: X_hvg
    output_space: gene
    pert_rep: onehot
    basal_rep: sample
    num_workers: 8
    pin_memory: true
    n_basal_samples: 1
    basal_mapping_strategy: random
    should_yield_control_cells: true
    batch_col: gem_group
    pert_col: drugname_drugconc
    cell_type_key: cell_line
    control_pert: ('DMSO_TF', 0.0, 'uM')
    map_controls: true
    perturbation_features_file: null
    store_raw_basal: false
    int_counts: false
    barcode: true
  output_dir: null
  debug: true
model:
  name: state
  checkpoint: null
  device: cuda
  kwargs:
    cell_set_len: 512
    blur: 0.05
    hidden_dim: 768
    loss: energy
    confidence_token: false
    n_encoder_layers: 1
    n_decoder_layers: 1
    predict_residual: true
    softplus: true
    freeze_pert_backbone: false
    transformer_decoder: false
    finetune_vci_decoder: false
    residual_decoder: false
    batch_encoder: false
    use_batch_token: false
    nb_decoder: false
    mask_attn: false
    use_effect_gating_token: false
    distributional_loss: energy
    init_from: null
    mmd_num_chunks: 1
    randomize_mmd_chunks: false
    transformer_backbone_key: llama
    transformer_backbone_kwargs:
      bidirectional_attention: true
      max_position_embeddings: ${model.kwargs.cell_set_len}
      hidden_size: ${model.kwargs.hidden_dim}
      intermediate_size: 3072
      num_hidden_layers: 8
      num_attention_heads: 12
      num_key_value_heads: 12
      head_dim: 64
      use_cache: false
      attention_dropout: 0.0
      hidden_dropout: 0.0
      layer_norm_eps: 1.0e-06
      pad_token_id: 0
      bos_token_id: 1
      eos_token_id: 2
      tie_word_embeddings: false
      rotary_dim: 0
      use_rotary_embeddings: false
    lora:
      enable: false
      r: 16
      alpha: 32
      dropout: 0.05
      bias: none
      target: auto
      adapt_mlp: false
      task_type: FEATURE_EXTRACTION
      merge_on_eval: false
training:
  wandb_track: false
  weight_decay: 0.0005
  batch_size: 16
  lr: 0.0001
  max_steps: 40000
  train_seed: 42
  val_freq: 2000
  ckpt_every_n_steps: 2000
  gradient_clip_val: 10
  gradient_accumulation_steps: 1
  loss_fn: mse
  devices: 1
  strategy: auto
  use_mfu: true
  mfu_kwargs:
    available_flops: 60000000000000.0
    use_backward: true
    logging_interval: 10
    window_size: 2
  cumulative_flops_use_backward: true
wandb:
  entity: alphasteen-beijing-university-of-posts-and-telecommunications
  project: state_training
  local_wandb_dir: /data3/fanpeishan/state/for_state/run_results/local_wandb_dir
  tags: []
name: debug
output_dir: /data3/fanpeishan/state/for_state/run_results/run27/configs_dir
use_wandb: false
overwrite: false
return_adatas: false
pred_adata_path: null
true_adata_path: null


Usage examples:
  Override single parameter:
    uv run state tx train data.batch_size=64

  Override nested parameter:
    uv run state tx train model.kwargs.hidden_dim=512

  Override multiple parameters:
    uv run state tx train data.batch_size=64 training.lr=0.001

  Change config group:
    uv run state tx train data=custom_data model=custom_model

Available config groups:
  data: default, perturbation
  wandb: default
  model: globalsimplesum, pertsets, decoder_only, scgpt-chemical, scgpt-genetic, perturb_mean, scvi, tahoe_llama_62089464, context_mean, embedsum, state_sm, tahoe_llama_212693232, pseudobulk, state, tahoe_best, old_neuralot, state_lg, celltypemean, cpa
  training: default, scvi, scgpt, cpa
```

2. state tx predict

```bash
usage: state tx predict [-h] --output-dir OUTPUT_DIR [--toml TOML] [--checkpoint CHECKPOINT] [--test-time-finetune TEST_TIME_FINETUNE] [--profile {full,minimal,de,anndata}] [--predict-only] [--shared-only] [--eval-train-data]

options:
  -h, --help            show this help message and exit
  --output-dir OUTPUT_DIR
                        Path to the output_dir containing the config.yaml file that was saved during training.
  --toml TOML           Optional path to a TOML data config to use instead of the training config.
  --checkpoint CHECKPOINT
                        Checkpoint filename. Default is 'last.ckpt'. Relative to the output directory.
  --test-time-finetune TEST_TIME_FINETUNE
                        If >0, run test-time fine-tuning for the specified number of epochs on only control cells.
  --profile {full,minimal,de,anndata}
                        run all metrics, minimal, only de metrics, or only output adatas
  --predict-only        If set, only run prediction without evaluation metrics.
  --shared-only         If set, restrict predictions/evaluation to perturbations shared between train and test (train ∩ test).
  --eval-train-data     If set, evaluate the model on the training data rather than on the test data.
```

3. state tx infer

```bash
usage: state tx infer [-h] [--checkpoint CHECKPOINT] --adata ADATA [--embed-key EMBED_KEY] [--pert-col PERT_COL] [--output OUTPUT] --model-dir MODEL_DIR [--celltype-col CELLTYPE_COL] [--celltypes CELLTYPES] [--batch-col BATCH_COL] [--control-pert CONTROL_PERT]
                      [--seed SEED] [--max-set-len MAX_SET_LEN] [--quiet] [--tsv TSV] [--lora-path LORA_PATH] [--lora-rank LORA_RANK] [--lora-alpha LORA_ALPHA] [--all-perts] [--virtual-cells-per-pert VIRTUAL_CELLS_PER_PERT] [--min-cells MIN_CELLS]
                      [--max-cells MAX_CELLS]

options:
  -h, --help            show this help message and exit
  --checkpoint CHECKPOINT
                        Path to model checkpoint (.ckpt). If not provided, defaults to model_dir/checkpoints/final.ckpt
  --adata ADATA         Path to input AnnData file (.h5ad)
  --embed-key EMBED_KEY
                        Key in adata.obsm for input features (if None, uses adata.X). If provided, .X will be left untouched in the output file.
  --pert-col PERT_COL   Column in adata.obs for perturbation labels
  --output OUTPUT       Path to output file (.h5ad or .npy). Defaults to <input>_simulated.h5ad
  --model-dir MODEL_DIR
                        Path to the training run directory. Must contain config.yaml, var_dims.pkl, pert_onehot_map.pt, batch_onehot_map.pkl.
  --celltype-col CELLTYPE_COL
                        Column in adata.obs to group by (defaults to auto-detected cell type column).
  --celltypes CELLTYPES
                        Comma-separated list of cell types to include (optional).
  --batch-col BATCH_COL
                        Batch column name in adata.obs. If omitted, tries config['data']['kwargs']['batch_col'] then common fallbacks.
  --control-pert CONTROL_PERT
                        Override the control perturbation label. If omitted, read from config; for 'drugname_drugconc', defaults to "[('DMSO_TF', 0.0, 'uM')]".
  --seed SEED           Random seed for control sampling (default: 42)
  --max-set-len MAX_SET_LEN
                        Maximum set length per forward pass. If omitted, uses the model's trained cell_set_len.
  --quiet               Reduce logging verbosity.
  --tsv TSV             Path to TSV file with columns 'perturbation' and 'num_cells' to pad the adata with additional perturbation cells copied from random controls.
  --lora-path LORA_PATH
                        Path to LoRA weights file (.pth) or directory containing lora_state.pth. If omitted, uses the base model without LoRA.
  --lora-rank LORA_RANK
                        LoRA rank (r). If omitted, inferred from saved weights.
  --lora-alpha LORA_ALPHA
                        LoRA alpha scaling factor. If omitted, inferred from saved weights.
  --all-perts           If set, add virtual copies of control cells for every perturbation in the saved one-hot map so all perturbations are simulated.
  --virtual-cells-per-pert VIRTUAL_CELLS_PER_PERT
                        When using --all-perts, limit the number of control cells cloned for each virtual perturbation to this many (default: use all available controls).
  --min-cells MIN_CELLS
                        Ensure each perturbation has at least this many cells by padding with virtual controls (if needed).
  --max-cells MAX_CELLS
                        Upper bound on cells per perturbation after padding; subsamples excess cells if necessary.
```

4. state tx preprocess_train

```bash
usage: state tx preprocess_train [-h] --adata ADATA --output OUTPUT --num_hvgs NUM_HVGS

options:
  -h, --help           show this help message and exit
  --adata ADATA        Path to input AnnData file (.h5ad)
  --output OUTPUT      Path to output preprocessed AnnData file (.h5ad)
  --num_hvgs NUM_HVGS  Number of highly variable genes to select
```

5. state tx preprocess_infer

```bash
usage: state tx preprocess_infer [-h] --adata ADATA --output OUTPUT --control-condition CONTROL_CONDITION --pert-col PERT_COL [--seed SEED] [--embed-key EMBED_KEY]

options:
  -h, --help            show this help message and exit
  --adata ADATA         Path to input AnnData file (.h5ad)
  --output OUTPUT       Path to output preprocessed AnnData file (.h5ad)
  --control-condition CONTROL_CONDITION
                        Control condition identifier (e.g., "[('DMSO_TF', 0.0, 'uM')]")
  --pert-col PERT_COL   Column name containing perturbation information (e.g., 'drugname_drugconc')
  --seed SEED           Random seed for reproducibility (default: 42)
  --embed-key EMBED_KEY
                        obsm key to use/replace instead of X (e.g., 'X_pca')
```

## state emb 命令

1. state emb fit

```bash
usage: state emb fit [-h] [--conf CONF] [hydra_overrides ...]

positional arguments:
  hydra_overrides  Hydra configuration overrides (e.g., embeddings.current=esm2-cellxgene)

options:
  -h, --help       show this help message and exit
  --conf CONF      Path to config YAML file
```

2. state emb transform

```bash
usage: state emb transform [-h] [--model-folder MODEL_FOLDER] [--checkpoint CHECKPOINT] [--config CONFIG] --input INPUT [--output OUTPUT] [--embed-key EMBED_KEY] [--protein-embeddings PROTEIN_EMBEDDINGS] [--lancedb LANCEDB] [--lancedb-update]
                           [--lancedb-batch-size LANCEDB_BATCH_SIZE] [--batch-size BATCH_SIZE]

options:
  -h, --help            show this help message and exit
  --model-folder MODEL_FOLDER
                        Path to the model checkpoint folder (required if --checkpoint is not provided)
  --checkpoint CHECKPOINT
                        Path to the specific model checkpoint (required if --model-folder is not provided)
  --config CONFIG       Path to config override. If omitted, uses the config embedded in the checkpoint; ignores any config in the model folder.
  --input INPUT         Path to input anndata file (h5ad)
  --output OUTPUT       Path to output file. If the filename ends with .h5ad, writes an embedded AnnData file with embeddings stored under --embed-key. If the filename ends with .npy, writes only the embeddings matrix as a NumPy array (no .h5ad is written).
  --embed-key EMBED_KEY
                        Name of key to store embeddings in the output AnnData (when --output is .h5ad) and the embedding key used for LanceDB storage (when --lancedb is set).
  --protein-embeddings PROTEIN_EMBEDDINGS
                        Path to protein embeddings override (.pt). If omitted, the CLI will look for 'protein_embeddings.pt' in --model-folder, then fall back to embeddings packaged in the checkpoint, and finally the path from the config.
  --lancedb LANCEDB     Path to LanceDB database for vector storage
  --lancedb-update      Update existing entries in LanceDB (default: append)
  --lancedb-batch-size LANCEDB_BATCH_SIZE
                        Batch size for LanceDB operations
  --batch-size BATCH_SIZE
                        Batch size for embedding forward pass (overrides config). Increase to use more VRAM and speed up embedding.
```

3. state emb query

```bash
usage: state emb query [-h] --lancedb LANCEDB --input INPUT --output OUTPUT [--k K] [--embed-key EMBED_KEY] [--exclude-distances] [--filter FILTER] [--batch-size BATCH_SIZE]

options:
  -h, --help            show this help message and exit
  --lancedb LANCEDB     Path to existing LanceDB database
  --input INPUT         Path to input anndata file with query cells
  --output OUTPUT       Path to output file for results (csv, parquet)
  --k K                 Number of nearest neighbors to return
  --embed-key EMBED_KEY
                        Key containing embeddings in input file
  --exclude-distances   Exclude vector distances in results
  --filter FILTER       Filter expression (e.g., 'cell_type=="B cell"')
  --batch-size BATCH_SIZE
                        Batch size for query operations
```

4. state emb preprocess

```bash
usage: state emb preprocess [-h] --profile-name PROFILE_NAME --train-csv TRAIN_CSV --val-csv VAL_CSV --output-dir OUTPUT_DIR [--config-file CONFIG_FILE] [--all-embeddings ALL_EMBEDDINGS] [--num-threads NUM_THREADS]

options:
  -h, --help            show this help message and exit
  --profile-name PROFILE_NAME
                        Name for the new profile (used for both embeddings and dataset)
  --train-csv TRAIN_CSV
                        Path to training CSV file (species,path,names columns)
  --val-csv VAL_CSV     Path to validation CSV file (species,path,names columns)
  --output-dir OUTPUT_DIR
                        Directory to output generated files
  --config-file CONFIG_FILE
                        Config file to update (default: src/state/configs/state-defaults.yaml)
  --all-embeddings ALL_EMBEDDINGS
                        Path to existing all_embeddings.pt file (if not provided, creates one-hot embeddings)
  --num-threads NUM_THREADS
                        Number of parallel worker processes to use (default: 1)
```

5. state emb eval

```bash
usage: state emb eval [-h] --checkpoint CHECKPOINT --adata ADATA [--config CONFIG] [--pert-col PERT_COL] [--control-pert CONTROL_PERT] [--gene-column GENE_COLUMN] [--batch-size BATCH_SIZE] [--protein-embeddings PROTEIN_EMBEDDINGS]

options:
  -h, --help            show this help message and exit
  --checkpoint CHECKPOINT
                        Path to model checkpoint file
  --adata ADATA         Path to AnnData file
  --config CONFIG       Path to configuration override. If omitted, uses the config embedded in the checkpoint.
  --pert-col PERT_COL   Column name for perturbation labels (default: gene)
  --control-pert CONTROL_PERT
                        Control perturbation label (default: non-targeting)
  --gene-column GENE_COLUMN
                        Column name for gene names (default: gene_name)
  --batch-size BATCH_SIZE
                        Batch size for model inference (overrides config default)
  --protein-embeddings PROTEIN_EMBEDDINGS
                        Path to protein embeddings override (.pt). If omitted, uses embeddings packaged in the checkpoint, or the path from config as fallback.
```

