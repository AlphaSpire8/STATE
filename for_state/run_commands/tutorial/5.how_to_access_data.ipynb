{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "* This is a tutorial on using Python for accessing the Tahoe-100M dataset hosted by the Arc Institute.\n",
    "* The data can be streamed or downloaded locally.\n",
    "  * For small jobs (e.g., summarizing the some metadata), streaming is recommended.\n",
    "  * For large jobs (e.g., training a model), downloading is recommended.\n",
    "* See the [README](README.md#obs-cell-metadata) for a description of the obs metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "If needed, install the necessary dependencies.\n",
    "\n",
    "You can use the [conda environment](../conda_envs/python.yml) provided in this git repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import pyarrow.dataset as ds\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize GCS file system for reading data from GCS\n",
    "fs = gcsfs.GCSFileSystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS bucket path\n",
    "gcp_base_path = \"gs://arc-ctc-tahoe100/2025-02-25/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obs metadata\n",
    "\n",
    "* `obs` â‰ƒ cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-sample\n",
    "\n",
    "* Useful for quickly summarizing the per-sample metadata (a small file versus the entire obs metadata file; see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to sample metadata\n",
    "infile = \"/\".join([gcp_base_path.rstrip(\"/\"), 'metadata', 'sample_metadata.parquet'])\n",
    "infile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read just the first 3 rows\n",
    "sample_metadata = ds.dataset(infile, filesystem=fs, format=\"parquet\").head(3).to_pandas()\n",
    "sample_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select certain columns and row filtering\n",
    "columns_to_read = ['sample', 'plate', 'mean_gene_count']  # Specify the columns you need\n",
    "dataset = ds.dataset(infile, filesystem=fs, format=\"parquet\")\n",
    "sample_metadata = dataset.to_table(filter=(ds.field('mean_gene_count') > 2000), columns=columns_to_read).to_pandas()\n",
    "sample_metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of samples\n",
    "columns_to_read = [\"sample\"]  # Specify the columns you need\n",
    "dataset = ds.dataset(infile, filesystem=fs, format=\"parquet\")\n",
    "sample_count = dataset.to_table(columns=columns_to_read).to_pandas()[\"sample\"].nunique()\n",
    "print(f\"Number of samples: {sample_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get samples per plate\n",
    "columns_to_read = [\"plate\", \"sample\"]  # Specify the columns you need\n",
    "dataset = ds.dataset(infile, filesystem=fs, format=\"parquet\")\n",
    "samples_per_plate = dataset.to_table(columns=columns_to_read).to_pandas().groupby(\"plate\").size()\n",
    "samples_per_plate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-observation\n",
    "\n",
    "* `obs` ~= cells\n",
    "* For the sake of this tutorial, we will just pull the first 100000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the obs_metadata file\n",
    "infile = \"/\".join([gcp_base_path.rstrip(\"/\"), 'metadata', 'obs_metadata.parquet'])\n",
    "infile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a subset of the metadata\n",
    "obs_metadata = ds.dataset(infile, filesystem=fs, format=\"parquet\").head(100000).to_pandas()\n",
    "obs_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample count\n",
    "obs_metadata[\"sample\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene count distribution\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "obs_metadata[\"gene_count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tscp (UMI) count distribution\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "obs_metadata[\"tscp_count\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in h5ad files\n",
    "\n",
    "* For this tutorial, we will be reading in a subsampled version of 1 h5ad file, since the per-plate h5ad files are rather large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the plate metadata file\n",
    "infile = \"gs://arc-ctc-tahoe100/2025-02-25/tutorial/plate3_2k-obs.h5ad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the h5ad file\n",
    "with fs.open(infile, 'rb') as f:\n",
    "    adata = sc.read_h5ad(f)\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the obs metadata\n",
    "print(adata.obs.shape)\n",
    "adata.obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next steps\n",
    "\n",
    "You can then use the anndata object for various downsteam analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading files\n",
    "\n",
    "You can use [gsutil](https://cloud.google.com/storage/docs/gsutil) to download any of the files in the bucket\n",
    "and work with them locally. \n",
    "\n",
    "Please be considerate to the [cost of egress](https://cloud.google.com/storage/pricing) when download the data from Google Cloud Storage.\n",
    "\n",
    "For example:\n",
    "\n",
    "```bash\n",
    "gsutil cp gs://arc-ctc-tahoe100/2025-02-25/tutorial/plate3_2k-obs.h5ad .\n",
    "```\n",
    "\n",
    "For large data transfers, it is better to use `gsutil rsync`:\n",
    "\n",
    "```bash\n",
    "gsutil rsync gs://arc-ctc-tahoe100/2025-02-25/tutorial/ .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Session Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scbasecount-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
